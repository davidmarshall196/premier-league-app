import pandas as pd
import numpy as np
import joblib
import sys
sys.path.append('../')

from premier_league import constants
if constants.RUN_DATA_EXPECTATIONS:
    from expectations_helpers import (
        AutoGreatExpectations,
        view_full_suite,
        view_suite_summary,
        save_expectations,
        load_expectations,
        validate_data
    )
from premier_league import (
    preprocessing,
    preprocessing_helpers,
    training,
    evaluation,
    prediction,
    data_extraction,
    visualisations,
    s3_helpers,
    postgres
)


import importlib
importlib.reload(constants)



df = s3_helpers.grab_data_s3(constants.TRAINING_DATA_LOCATION)
df = df[constants.COLUMNS_REQ]


df.head()





if constants.RUN_DATA_EXPECTATIONS:
    ge_class = AutoGreatExpectations(df)


if constants.RUN_DATA_EXPECTATIONS:
    ge_data = ge_class.generate_expectations(verbose=False)


if constants.RUN_DATA_EXPECTATIONS:
    view_suite_summary(ge_data)





if constants.RUN_DATA_EXPECTATIONS:
    save_expectations(ge_data, constants.exp_loc)


if constants.RUN_DATA_EXPECTATIONS:
    data_expectations = load_expectations(constants.exp_loc)


if constants.INITIAL_DATA_LOAD:
    full_data = data_extraction.load_all_data(
        constants.TRAINING_DATA_LOCATION,
        constants.COLUMNS_REQ
    )
else:
    full_data = s3_helpers.grab_data_s3(
        constants.TRAINING_DATA_LOCATION)



full_data = data_extraction.add_new_data(
    full_data, 
    constants.COLUMNS_REQ,
    constants.TRAINING_DATA_LOCATION
)


if constants.RUN_DATA_EXPECTATIONS:
    validation_results = validate_data(full_data, data_expectations)





df = s3_helpers.grab_data_s3(constants.TRAINING_DATA_LOCATION)
df = df[constants.COLUMNS_REQ]
df.head()


transformers = preprocessing.fit_transformers(
    df
)


s3_helpers.save_transformer_s3_pickle(
    transformers, 
    constants.TRANSFORMER_PATH
)


transformed_data = preprocessing.transform_data(
    df, transformers
)


#transformed_data = transformed_data.drop(['HomeTeam', 'AwayTeam', 'season'], axis=1)


training_data, testing_data = preprocessing.split_data(transformed_data)


training_data.shape


testing_data.shape


training_data.head(2)





target_column = 'FTR'


hyperparameters = training.optimise_hyperparameters(
    training_data,
    target_column,
    max_evals = constants.MAX_EVALS
)


print(hyperparameters)


#Â Start postgresDB for model logging
postgres.start_rds_instance(constants.POSTGRES_DB_ID)


classifier, run_id = training.train_model(
    training_data[[col for col in training_data if col != target_column]],
    training_data[target_column],
    hyperparameters = hyperparameters
)


s3_helpers.save_transformer_s3_pickle(
    classifier, 
    constants.CLASS_MODEL_NAME,
    is_transformer=False
)





y_test = testing_data[target_column]
x_test = testing_data[[col for col in testing_data if col != target_column]]


predictions = prediction.predict(x_test, classifier)





evaluation_metrics = evaluation.evaluate_model(
    predictions, 
    y_test,
    model_type='result',
    run_id=run_id
)


evaluation_metrics





transformed_data = prediction.add_match_result(
    transformed_data, classifier, df )


s3_helpers.save_data_s3(
    transformed_data,
    constants.TRANSFORMED_DATA_LOCATION
)


transformed_data.shape


training_data, testing_data = preprocessing.split_data(transformed_data)


hyperparameters = training.optimise_hyperparameters(
    training_data.drop(['FTR','FTAG'], axis=1),
    'FTHG',
    classification=False,
    max_evals=constants.MAX_EVALS
)


regressor_1, run_id_home = training.train_model(
    training_data.drop(['FTR', 'FTHG','FTAG'], axis=1),
    training_data['FTHG'],
    model_type='home',
    verbose=False,
    hyperparameters = hyperparameters
)


s3_helpers.save_transformer_s3_pickle(
    regressor_1, 
    constants.HOME_MODEL_NAME,
    is_transformer=False
)


hyperparameters = training.optimise_hyperparameters(
    training_data.drop(['FTR', 'FTHG'], axis=1),
    'FTAG',
    classification=False,
    max_evals=constants.MAX_EVALS
)


regressor_2, run_id_away = training.train_model(
    training_data.drop(['FTR', 'FTHG','FTAG'], axis=1),
    training_data['FTAG'],
    model_type='away',
    verbose=False,
    hyperparameters = hyperparameters
)


s3_helpers.save_transformer_s3_pickle(
    regressor_2, 
    constants.AWAY_MODEL_NAME,
    is_transformer=False
)





y_test = testing_data['FTHG']
x_test = testing_data.copy()


predictions_1 = prediction.predict(x_test, regressor_1)


evaluation_metrics = evaluation.evaluate_model(
    predictions_1, 
    y_test, 
    model_type='home',
    run_id=run_id_home    
)
evaluation_metrics





y_test = testing_data['FTAG']
x_test = testing_data.copy()


predictions_2 = prediction.predict(
    x_test, classifier)


evaluation_metrics = evaluation.evaluate_model(
    predictions_2, 
    y_test, 
    model_type='away',
    run_id=run_id_away
)
evaluation_metrics





x_test['Home Prediction'] = predictions_1
x_test['Away Prediction'] = predictions_2


visualisations.histoplot(x_test, ha='Home')


visualisations.histoplot(x_test, ha='Away')


visualisations.actuals_predicted(x_test, ha='Home')


visualisations.actuals_predicted(x_test, ha='Away')


visualisations.plot_features(regressor_1, 'Home',n=20)


visualisations.plot_features(regressor_2, 'Away')


import mlflow




